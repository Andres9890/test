name: Archive Web Crawl

on:
  workflow_dispatch:
    inputs:
      url:
        description: 'Target website URL to crawl'
        required: true

jobs:
  crawl-and-upload:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Python & IA CLI
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'
    - run: |
        python -m pip install --upgrade pip
        pip install internetarchive

    - name: Prepare identifiers
      shell: bash
      run: |
        URL="${{ github.event.inputs.url }}"
        # extract the domain (e.g. example.com)
        DOMAIN="$(echo "${URL}" | awk -F[/:] '{print $4}')"
        # timestamp in UTC
        TS="$(date -u +%Y%m%dT%H%M%SZ)"
        # combine to form item identifier
        ID="${DOMAIN}_${TS}"
        echo "ITEM_ID=${ID}" >> $GITHUB_ENV
        echo "WARC_FILE=${ID}.warc.gz" >> $GITHUB_ENV

    - name: Mirror site to WARC
      shell: bash
      run: |
        wget \
          --mirror \
          --convert-links \
          --adjust-extension \
          --page-requisites \
          --no-parent \
          --warc-file="${ITEM_ID}" \
          --warc-cdx \
          --warc-compression=gzip \
          "${{ github.event.inputs.url }}"

    - name: Upload to Archive.org
      env:
        IA_ACCESS_KEY_ID: ${{ secrets.IA_ACCESS_KEY_ID }}
        IA_SECRET_ACCESS_KEY: ${{ secrets.IA_SECRET_ACCESS_KEY }}
      shell: bash
      run: |
        ia upload "${ITEM_ID}" "${WARC_FILE}" \
          --metadata="title:${ITEM_ID} crawl" \
          --metadata="mediatype:web" \
          --retries=3 --continue
